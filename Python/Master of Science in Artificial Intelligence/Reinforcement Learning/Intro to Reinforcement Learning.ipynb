{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NxgP4gIN5eF"
   },
   "source": [
    "**Recycling robot example** (from Sutton, page 42)\n",
    "References:\n",
    "  - Gym documentation: https://gym.openai.com/\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fQ-0sEtFFcTM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.envs.toy_text import discrete\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPaZiYtu6aX6"
   },
   "source": [
    "# Consider the robot model described in Barto and Sutton Example 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "U96qJdswGBFr"
   },
   "outputs": [],
   "source": [
    "states = [\"high\", \"low\"]\n",
    "actions = [\"wait\", \"search\", \"recharge\"]\n",
    "\n",
    "P = {}\n",
    "\n",
    "P[0] = {}\n",
    "P[1] = {}\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "r_wait = 0.5\n",
    "r_search = 2.0\n",
    "\n",
    "# We define a discrete environment with the corresponding transitions\n",
    "def gen_ambient(alpha=alpha, beta=beta, r_wait=r_wait, r_search=r_search):\n",
    "    P[0][0] = [(1.0, 0, r_wait, False)]\n",
    "    P[0][1] = [(alpha, 0, r_search, False),\n",
    "               (1-alpha, 1, r_search, False)]\n",
    "    P[0][2] = [(1,0,0,False)]\n",
    "\n",
    "    P[1][0] = [(1.0, 1, r_wait, False)]\n",
    "    P[1][1] = [(beta, 1, r_search, False), \n",
    "               (1-beta, 0, -3.0, False)]\n",
    "    P[1][2] = [(1.0, 0, 0.0, False)]\n",
    "    env = discrete.DiscreteEnv(2, 3, P, [0.0, 1.0])\n",
    "    return(env)\n",
    "\n",
    "env = gen_ambient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvcF7--Z6aX8"
   },
   "source": [
    "# Implement the random strategy for 20 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoNiKgvOIC3n"
   },
   "source": [
    "Define a random action and see what reward it produces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation= 1\n",
      "\n",
      "\n",
      "Iteration  1\n",
      "observation= 1\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  2\n",
      "observation= 0\n",
      "reward= 0.0\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  3\n",
      "observation= 0\n",
      "reward= 0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  4\n",
      "observation= 0\n",
      "reward= 2.0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  5\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  6\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  7\n",
      "observation= 0\n",
      "reward= 2.0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  8\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  9\n",
      "observation= 0\n",
      "reward= 2.0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  10\n",
      "observation= 0\n",
      "reward= 2.0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  11\n",
      "observation= 0\n",
      "reward= 0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  12\n",
      "observation= 0\n",
      "reward= 2.0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  13\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  14\n",
      "observation= 0\n",
      "reward= 2.0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  15\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  16\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  17\n",
      "observation= 0\n",
      "reward= 2.0\n",
      "done= False\n",
      "info= {'prob': 1}\n",
      "\n",
      "\n",
      "Iteration  18\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  19\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n",
      "Iteration  20\n",
      "observation= 0\n",
      "reward= 0.5\n",
      "done= False\n",
      "info= {'prob': 1.0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation_init = env.reset()\n",
    "print(\"Initial Observation=\",observation_init)\n",
    "print(\"\\n\")\n",
    "cum_reward = 0\n",
    "cumulative_reward = [0]\n",
    "\n",
    "for t in range(20):\n",
    "\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    cum_reward += reward\n",
    "    cumulative_reward.append(cum_reward)\n",
    "    \n",
    "    print(\"Iteration \",t+1)\n",
    "    print(\"observation=\",observation)\n",
    "    print(\"reward=\",reward)\n",
    "    print(\"done=\",done)\n",
    "    print(\"info=\",info)\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break    \n",
    "    \n",
    "    print(\"\\n\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9kpyAEk6aYB"
   },
   "source": [
    "# Plot the global reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of time steps\n",
    "t = list(range(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x18f7df39e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting cumulative reward\n",
    "plt.scatter(t,cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC0PJOkV6aYH"
   },
   "source": [
    "# Compute directly the optimal value function for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha = 1\n",
    "beta = 1\n",
    "r_wait = 0.5\n",
    "r_search = 2.0\n",
    "\n",
    "1) State Low\n",
    "\n",
    "val1 = $beta[r search + gamma.V*(low)] + (1-beta)[-3 + gamma.V*(high)]$ ---> (search)\n",
    "\n",
    "val2 = $1[r wait + gamma.V*(low)]$ ---> (wait)\n",
    "\n",
    "val3 = $1[0 + gamma.V*(high)]$ ---> (recharge)\n",
    "\n",
    "V*(low) = max{val1, val2, val3}\n",
    "\n",
    "2) State High\n",
    "\n",
    "val1 = $alpha[r search + gamma.V*(high)] + (1-alpha)[r search + gamma.V*(low)]$ ---> (search)\n",
    "\n",
    "val2 = $1[r wait + gamma.V*(high)]$ ---> (wait)\n",
    "\n",
    "V*(high) = max{val1, val2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining important parameters\n",
    "gamma = 0.9\n",
    "theta = 0.1\n",
    "max_iterations=1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRz0uMX1LbXz"
   },
   "source": [
    "# Implement Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVuBLOrWL1ru"
   },
   "source": [
    "  Evaluate the optimal value function given a full description of the environment dynamics\n",
    "  \n",
    "  \n",
    "\n",
    "```\n",
    " Args:\n",
    "\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "  \n",
    "  Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "```\n",
    "\n",
    "\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env):\n",
    "\n",
    "    #Initialize state-value function with zeros for each environment state\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(int(max_iterations)):    \n",
    "\n",
    "        #Early stopping condition\n",
    "        delta = 0\n",
    "        #Finding optimal value function\n",
    "        for s in range(env.nS):\n",
    "\n",
    "            #Find the best action for this state\n",
    "            action_value = np.zeros(env.nA)\n",
    "            for action in range(env.nA):\n",
    "                for probability, next_state, reward, terminated in env.P[s][action]:\n",
    "                    action_value[action] += probability*(reward + gamma*V[next_state])\n",
    "\n",
    "            #Select best action to perform based on the highest state-action value\n",
    "            best_action_value = np.max(action_value)        \n",
    "\n",
    "            #Calculate change in value\n",
    "            delta = max(delta, np.abs(V[s] - best_action_value))\n",
    "            #Update the value function for current state\n",
    "            V[s] = best_action_value\n",
    "\n",
    "        #Check if we can stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    #Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        #Find the best action for this state\n",
    "        action_value = np.zeros(env.nA)\n",
    "        for action in range(env.nA):  \n",
    "            for probability, next_state, reward, terminated in env.P[s][action]:\n",
    "                action_value[action] += probability*(reward + gamma*V[next_state])      \n",
    "\n",
    "        #Select best action based on the highest state-action value\n",
    "        best_action = np.argmax(action_value)\n",
    "        #Update the policy to perform a better action at a current state\n",
    "        policy[s] = best_action        \n",
    "            \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGWxWGNA6aYJ"
   },
   "source": [
    "# Implement policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9o3S2kaKQKX"
   },
   "source": [
    "First a policy evluation\n",
    "\n",
    "```\n",
    "Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "        \n",
    "```\n",
    "\n",
    "Then a policy improvement:\n",
    "\n",
    "\n",
    "```\n",
    " Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iter(env):\n",
    "\n",
    "    #Initialize policy and value arbitrarily\n",
    "    policy = [0 for s in range(env.nS)]\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    is_value_changed = True\n",
    "    iterations = 0\n",
    "    while is_value_changed:\n",
    "        is_value_changed = False\n",
    "        iterations += 1\n",
    "\n",
    "        #Policy Evaluation\n",
    "        # Repeat until change in value is below the threshold\n",
    "        for i in range(int(max_iterations)):    \n",
    "\n",
    "            # Initialize a change of value function as zero\n",
    "            delta = 0\n",
    "            # run value iteration for each state\n",
    "            for s in range(env.nS):\n",
    "                # Initial a new value of current state\n",
    "                v = 0        \n",
    "                # Calculate the expected value\n",
    "                for probability, next_state, reward, terminated in env.P[s][policy[s]]:\n",
    "                    v += probability*(reward + gamma*V[next_state])            \n",
    "                # Calculate the absolute change of value function\n",
    "                delta = max(delta, np.abs(V[s] - v))\n",
    "                # Update value function\n",
    "                V[s] = v\n",
    "\n",
    "            # Terminate if value change is insignificant\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        #Policy Improvement\n",
    "        for s in range(env.nS):\n",
    "            q_best = V[s]\n",
    "            for a in range(env.nA):\n",
    "                q_sa = 0\n",
    "                for probability, next_state, reward, terminated in env.P[s][a]:\n",
    "                    q_sa += probability*(reward + gamma*V[next_state])   \n",
    "\n",
    "                if q_sa > q_best:\n",
    "                    policy[s] = a\n",
    "                    q_best = q_sa\n",
    "                    is_value_changed = True\n",
    "\n",
    "        #print (\"Iterations:\", iterations)\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brOjUJvE6aYV"
   },
   "source": [
    "# Using the 3 algorithms do the following experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "jVKvrm0t6aYV"
   },
   "outputs": [],
   "source": [
    "exp1 = gen_ambient(alpha=0.9, beta=0.9, r_search=3, r_wait=2)\n",
    "exp2 = gen_ambient(alpha=0.8, beta=0.5, r_search=3, r_wait=2)\n",
    "exp3 = gen_ambient(alpha=0.5, beta=0.5, r_search=3, r_wait=2)\n",
    "exp4 = gen_ambient(alpha=0.9, beta=0.6, r_search=1, r_wait=0.9)\n",
    "exp5 = gen_ambient(alpha=0.9, beta=0.6, r_search=1, r_wait=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration: V = [13.2791185  11.95120665]\n",
      "Policy Iteration: V = [13.79310345 12.4137931 ]\n"
     ]
    }
   ],
   "source": [
    "exp_marco = gen_ambient(alpha=0.5, beta=0.4)\n",
    "\n",
    "V, policy = value_iter(exp_marco)\n",
    "print(\"Value Iteration: V =\",V)\n",
    "V, policy = policy_iter(exp_marco)\n",
    "print(\"Policy Iteration: V =\",V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration: V = [27.27134934 25.17163006]\n",
      "Policy Iteration: V = [28.07142857 25.92857143]\n"
     ]
    }
   ],
   "source": [
    "exp1 = gen_ambient(alpha=0.9, beta=0.9, r_search=3, r_wait=2)\n",
    "\n",
    "V, policy = value_iter(exp1)\n",
    "print(\"Value Iteration: V =\",V)\n",
    "V, policy = policy_iter(exp1)\n",
    "print(\"Policy Iteration: V =\",V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration: V = [24.75718575 22.28146717]\n",
      "Policy Iteration: V = [25.42372881 22.88135593]\n"
     ]
    }
   ],
   "source": [
    "exp2 = gen_ambient(alpha=0.8, beta=0.5, r_search=3, r_wait=2)\n",
    "\n",
    "V, policy = value_iter(exp2)\n",
    "print(\"Value Iteration: V =\",V)\n",
    "V, policy = policy_iter(exp2)\n",
    "print(\"Policy Iteration: V =\",V)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration: V = [20.94909811 19.1309167 ]\n",
      "Policy Iteration: V = [21.81818182 20.        ]\n"
     ]
    }
   ],
   "source": [
    "exp3 = gen_ambient(alpha=0.5, beta=0.5, r_search=3, r_wait=2)\n",
    "\n",
    "V, policy = value_iter(exp3)\n",
    "print(\"Value Iteration: V =\",V)\n",
    "V, policy = policy_iter(exp3)\n",
    "print(\"Policy Iteration: V =\",V)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration: V = [8.63491791 8.11370619]\n",
      "Policy Iteration: V = [9.52631579 9.        ]\n"
     ]
    }
   ],
   "source": [
    "exp4 = gen_ambient(alpha=0.9, beta=0.6, r_search=1, r_wait=0.9)\n",
    "\n",
    "V, policy = value_iter(exp4)\n",
    "print(\"Value Iteration: V =\",V)\n",
    "V, policy = policy_iter(exp4)\n",
    "print(\"Policy Iteration: V =\",V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration: V = [8.36147152 7.52532436]\n",
      "Policy Iteration: V = [9.17431193 8.25688073]\n"
     ]
    }
   ],
   "source": [
    "exp5 = gen_ambient(alpha=0.9, beta=0.6, r_search=1, r_wait=0.5)\n",
    "\n",
    "V, policy = value_iter(exp5)\n",
    "print(\"Value Iteration: V =\",V)\n",
    "V, policy = policy_iter(exp5)\n",
    "print(\"Policy Iteration: V =\",V) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAT_YZq56aYY"
   },
   "source": [
    "# Compare the different strategies with the random one\n",
    "# Compare the different strategies in terms of speed of convergence for the different scenarios\n",
    "# What would you do if alpha and beta are unknown (and you dont know RL)? Try to implement something if you have time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy Iteration involves two steps: policy evaluation and policy improvement. The two are repeated iteratively until policy convergence.\n",
    "- Value Iteration also involves two steps: finding optimal value function and one policy extraction. In this case, however, the two steps are not repeated iteratively because once the value function is optimal, then the policy obtained from it should also be optimal.\n",
    "- The algorithms for policy evaluation and finding the optimal value function are very similar, except that the latter has a max operator.\n",
    "- Even though each iteration of policy iteration is more computationally expensive, this method often takes fewer number of iterations to converge compared to value iteration."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP1_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
